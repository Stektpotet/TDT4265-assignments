{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "$\\gdef\\pdv#1#2{\\frac{\\partial #1}{\\partial #2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the gradient for Logistic Regression. To minimize the cost function with gradient\n",
    "descent, we require the gradient of the cost function. Show that for Equation 3 (Binary Cross Entropy Loss), the gradient is:\n",
    "$$\\pdv{C^n(w)}{w_i} = −\\left(y^n − \\hat{y}^n\\right)x^n_i$$\n",
    "\n",
    "when the output of our network is given by\n",
    "$$\\hat{y}=f(x)=\\frac{1}{1+e^{-w^Tx}},\\,w^Tx=\\sum_i^I w_i \\cdot x_i$$\n",
    "\n",
    "Where the cross entropy loss is defined as:\n",
    "$$C(w)=\\frac{1}{N}\\sum_{n=1}^N C^n(w)\\text{, where }C^n(w)=-\\left(y^n\\ln{\\left(\\hat{y}^n\\right)}+\\left(1-y^n\\right)\\ln{\\left(1-\\hat{y}^n\\right)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the chain rule, we have that\n",
    "$$\n",
    "\\pdv{C^n(w)}{w_i}=\\pdv{C^n(w)}{\\hat{y}^n}\\pdv{\\hat{y}^n}{w_i}\n",
    "$$\n",
    "\n",
    "We utilise the given hint $\\pdv{f(x^n)}{w_i}=x^n_i f(x^n)\\left(1-f(x^n)\\right)$, and identify that $f(x^n) \\equiv \\hat{y}^n$, such that our expression can be rewritten as\n",
    "\n",
    "$$\n",
    "\\pdv{C^n(w)}{w_i}=\\pdv{C^n(w)}{\\hat{y}^n}x^n_i \\hat{y}^n(1-\\hat{y}^n)\n",
    "$$\n",
    "\n",
    "Now, we only need to unravel the factor $\\pdv{C^n(w)}{\\hat{y}^n}$, and we'll be at the full gradient expression:\n",
    "\n",
    "$$\n",
    "\\pdv{C^n(w)}{\\hat{y}^n}\\text{, where } C^n(w)=-(y^n\\ln{(\\hat{y}^n)}+(1-y^n)\\ln{(1-\\hat{y}^n)})\n",
    "$$\n",
    "\n",
    "We start of by reorganising $C^n(w)$ to allow for easy derivation of individual terms:\n",
    "\n",
    "\\begin{align}\n",
    "C^n(w)  &=-(y^n\\ln{(\\hat{y}^n)}+(1-y^n)\\ln{(1-\\hat{y}^n)})\\\\\n",
    "        &=-(y^n\\ln{(\\hat{y}^n)}+(\\ln{(1-\\hat{y}^n)}-y^n\\ln{(1-\\hat{y}^n)}))\\\\\n",
    "        &=-(y^n\\ln{(\\hat{y}^n)}+\\ln{(1-\\hat{y}^n)}-y^n\\ln{(1-\\hat{y}^n)})\\\\\n",
    "        &=-y^n\\ln{(\\hat{y}^n)}-\\ln{(1-\\hat{y}^n)}+y^n\\ln{(1-\\hat{y}^n)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Now we can rewrite the partial derivative $\\pdv{C^n(w)}{\\hat{y}^n}$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{\\hat{y}^n}&=\\pdv{}{\\hat{y}^n}\\left(-y^n\\ln{(\\hat{y}^n)}\\right)&&+\\pdv{}{\\hat{y}^n}\\left(-\\ln{(1-\\hat{y}^n)}\\right)&&+\\pdv{}{\\hat{y}^n}\\left(y^n\\ln{(1-\\hat{y}^n)}\\right)\\\\\n",
    "\\end{align}\n",
    "\n",
    "We start by taking out the _'constants'_ ($y^n$ and term signs), before we proceed to evaluate each individual derivative:\n",
    "\n",
    "\\begin{align}\n",
    "&=\\color{blue}{-y^n}\\pdv{}{\\hat{y}^n}\\left(\\ln{\\hat{y}^n}\\right)&&\n",
    "\\color{blue}{-1}\\pdv{}{\\hat{y}^n}\\left(\\ln{(1-\\hat{y}^n)}\\right)&&\n",
    "\\color{blue}{+y^n}\\pdv{}{\\hat{y}^n}\\left(\\ln{(1-\\hat{y}^n)}\\right)\\\\\n",
    "%SPACING\n",
    "&=-y^n\\color{blue}{\\frac{1}{\\hat{y}^n}}&&\n",
    "-1\\color{blue}{\\frac{1}{1-\\hat{y}^n}\\pdv{}{\\hat{y}^n}\\left(1-\\hat{y}^n\\right)}&&\n",
    "+y^n\\color{blue}{\\frac{1}{1-\\hat{y}^n}\\pdv{}{\\hat{y}^n}\\left(1-\\hat{y}^n\\right)}\\\\\n",
    "%SPACING\n",
    "&=-y^n\\frac{1}{\\hat{y}^n}&&\n",
    "-1\\frac{1}{1-\\hat{y}^n}\\color{blue}{(-1)}&&\n",
    "+y^n\\frac{1}{1-\\hat{y}^n}\\color{blue}{(-1)}\\\\\n",
    "&=\\color{blue}{-\\frac{y^n}{\\hat{y}^n}}&&\n",
    "\\color{blue}{+\\frac{1}{1-\\hat{y}^n}}&&\n",
    "\\color{blue}{-\\frac{y^n}{1-\\hat{y}^n}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "In other words, we have that\n",
    "\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{\\hat{y}^n} &= -\\frac{y^n}{\\hat{y}^n} + \\frac{1}{1-\\hat{y}^n} - \\frac{y^n}{1-\\hat{y}^n}\\\\\n",
    "                        &=-\\frac{y^n(1-\\hat{y}^n)}{\\hat{y}^n(1-\\hat{y}^n)}+\\frac{\\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)}-\\frac{y^n\\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)}\\\\\n",
    "                        &=\\frac{\\hat{y}^n-y^n(1-\\hat{y}^n) - y^n\\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)}\\\\\n",
    "                        &=\\frac{\\hat{y}^n-y^n\\color{red}{+y^n\\hat{y}^n - y^n\\hat{y}^n}}{\\hat{y}^n(1-\\hat{y}^n)}\\\\\n",
    "                        &=\\frac{\\hat{y}^n-y^n}{\\hat{y}^n(1-\\hat{y}^n)}\\\\\n",
    "                        &=-\\frac{y^n-\\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the two partial derivative expressions we've computed; $\\pdv{C^n(w)}{\\hat{y}^n}$ and $\\pdv{\\hat{y}^n}{w_i}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{w_i} = \\pdv{C^n(w)}{\\hat{y}^n}\\pdv{\\hat{y}^n}{w_i} &= \\left(-\\frac{y^n-\\hat{y}^n}{\\hat{y}^n(1-\\hat{y}^n)}\\right)\\left(x^n_i \\hat{y}^n(1-\\hat{y}^n)\\right)\\\\\n",
    "&= \\left(-\\frac{y^n-\\hat{y}^n}{\\color{red}{\\hat{y}^n(1-\\hat{y}^n)}}\\right)\\left(x^n_i \\color{red}{\\hat{y}^n(1-\\hat{y}^n})\\right)\\\\\n",
    "&= -\\left(y^n - \\hat{y}^n\\right)x^n_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the gradient for Softmax Regression. For the multi-class cross entropy cost in\n",
    "Equation 5 (Categorical Cross Entropy Loss), show that the gradient is:\n",
    "\n",
    "$$\n",
    "\\pdv{C^n(w)}{w_{kj}}=-x^n_j\\left(y^n_k-\\hat{y}^n_k\\right)\n",
    "$$\n",
    "\n",
    "The Categorical Cross Entropy Loss function is given by\n",
    "\n",
    "$$\n",
    "C^n(w)=-\\sum^K_{k=1}y^n_k\\ln{(\\hat{y}^n_k)}\n",
    "$$\n",
    "\n",
    "Additionally, we have that $\\hat{y}$ is a vector, with elements $\\left\\{\\hat{y}_k\\right\\}^K_{k=1}$\n",
    "Each of which can be represented by\n",
    "\n",
    "$$\n",
    "\\hat{y}_k = \\frac{e^{z_k}}{\\sum^K_{k'}e^{z_{k'}}},\\,\\text{ where } z_k=w^T_k \\cdot x = \\sum^I_i w_{ki} \\cdot x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "C^n(w)&=-\\sum^K_{k=1}\\left[y^n_k\\ln{\\left(\\hat{y}^n_k\\right)}\\right]\n",
    "\\end{align}\n",
    "Let's begin by declaring the chain we have to get through...\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{w_{ij}}&=\\pdv{C^n(w)}{z_{i}}\\pdv{z_i}{w_{ij}}\n",
    "\\end{align}\n",
    "That is, the derivative of the cost (cross entropy loss) wrt. the network output vector element $z_i$ times the derivative of the output vector element wrt. to the corresponding weights. From here, let's begin with the first of the two. By the law of derivative summation, we have:\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{z_{i}}&=-\\sum^K_{k=1}\\left[ y^n_k \\pdv{\\ln{\\left( \\hat{y}^n_k \\right)}}{z_i} \\right]\n",
    "\\end{align}\n",
    "We utilize the logarithmic derivative ($\\pdv{\\ln{a}}{b}=\\frac{1}{a}\\pdv{a}{b}$), and get\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{z_{i}}&=-\\sum^K_{k=1}\\left[ \\frac{y^n_k}{\\hat{y}^n_k } \\pdv{\\hat{y}^n_k}{z_i} \\right]\n",
    "\\end{align}\n",
    "From here, let's focus on the inner partial derivative - the derivative of the softmax function wrt. the output vector element -  and leave the rest unchanged...\n",
    "\\begin{align}\n",
    "\\pdv{\\hat{y}^n_k}{z_i}\n",
    "=\\pdv{}{z_i}\\frac{e^{z_k}}{\\sum^K_{k'=1}\\left[e^{z_{k'}}\\right]}\n",
    "=\\pdv{}{z_i}\\frac{e^{z_k}}{\\left[e^{z_{1}} + \\ldots + e^{z_{i}} + \\ldots + e^{z_{K}}\\right]}&\\text{ For clarity, expand the summation}\n",
    "\\end{align}\n",
    "\n",
    "Before we carry on from this, we need to consider how this all behaves in relation to $i$ and $k$. $i$ and $k$ in the context of $z$ illustrates which element of the $z$-vector we are working with (technically, the $z^n$-vector as this expression also spans $n \\in N$, but for the sake of this computation this is merely a technicality). The point here, is that $i$ and $k$ may overlap, as they're indexing the same vector, and how we differentiate this expression boils down to evaluating the two cases of $k \\neq i$ and $i = k$.\n",
    "\n",
    "\\begin{align}\n",
    "\\pdv{\\hat{y}^n_k}{z_i}&=\\begin{cases}\n",
    "\\frac{0\\left[e^{z_{1}} + \\ldots + e^{z_{i}} + \\ldots + e^{z_{K}}\\right] - \\left[0 +  \\ldots + e^{z_{i}} + \\ldots + 0\\right]e^{z_{k}}}{\\left[e^{z_{1}} + \\ldots + e^{z_{i}} + \\ldots + e^{z_{K}}\\right]^2}& \\left\\{k \\neq i\\right\\}\\\\\\\\\n",
    "\\frac{e^{z_{i}}\\left[e^{z_{1}} + \\ldots + e^{z_{i}} + \\ldots + e^{z_{K}}\\right] - \\left[0 +  \\ldots + e^{z_{i}} + \\ldots + 0\\right]e^{z_{k}}}{\\left[e^{z_{1}} + \\ldots + e^{z_{i}} + \\ldots + e^{z_{K}}\\right]^2}& \\left\\{k = i\\right\\}\\\\\n",
    "\\end{cases}\\\\\n",
    "&=\\begin{cases}\n",
    "\\frac{0 - e^{z_{i}}e^{z_{k}}}{\\left[e^{z_{1}} + \\ldots + e^{z_{i}} + \\ldots + e^{z_{K}}\\right]^2}& \\hspace{5.34em}\\left\\{k \\neq i\\right\\}\\\\\\\\\n",
    "\\frac{e^{z_{i}}\\left[e^{z_{1}} + \\ldots + e^{z_{i}} + \\ldots + e^{z_{K}}\\right] - \\left(e^{z_{i}}\\right)^2}{\\left[e^{z_{1}} + \\ldots + e^{z_{i}} + \\ldots + e^{z_{K}}\\right]^2}& \\hspace{5.34em}\\left\\{k = i\\right\\}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Now that we've handled the derivatives of the two scenarios and handled the variable dependencies, we can again revert to the more concise summation form:\n",
    "\\begin{align}\n",
    "\\pdv{\\hat{y}^n_k}{z_i}&=\\begin{cases}\n",
    "\\frac{- e^{z_{i}}e^{z_{k}}}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]^2}& \\hspace{5.34em}\\left\\{k \\neq i\\right\\}\\\\\\\\\n",
    "\\frac{e^{z_{i}}\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right] - \\left(e^{z_{i}}\\right)^2}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]^2}& \\hspace{5.34em}\\left\\{k = i\\right\\}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "We're not quite done with it yet though, as we can go on and simplify even further, by recognising the hidden factors within these expressions:\n",
    "\\begin{align}\n",
    "\\pdv{\\hat{y}^n_k}{z_i}&=\\begin{cases}\\\\\n",
    "\\frac{- e^{z_{i}}e^{z_{k}}}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]^2}=-\\frac{ e^{z_{i}}}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]}\\frac{e^{z_{k}}}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]} = -\\hat{y}^n_{i}\\hat{y}^n_{k}  &\\left\\{k \\neq i\\right\\}\\\\\n",
    "\\\\\\\\\n",
    "\\frac{e^{z_{i}}\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right] - \\left(e^{z_{i}}\\right)^2}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]^2} = \\frac{e^{z_{i}}\\left(\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right] - e^{z_{i}}\\right)}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]^2}\\\\ = \\frac{e^{z_{i}}}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]}\\left(\\frac{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]} - \\frac{e^{z_{i}}}{\\sum^K_{k'=1} \\left[ e^{z_{k'}} \\right]}\\right) = \\hat{y}^n_{i}\\left(1-\\hat{y}^n_{i}\\right) &\\left\\{k = i\\right\\}\\\\\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "With that out of the way, lets get back to the original first part of the gradient chain:\n",
    "\n",
    "\\begin{align}\n",
    "    \\pdv{C^n(w)}{z_{i}}&=-\\sum^K_{k=1}\\left[ \\frac{y^n_k}{\\hat{y}^n_k } \\pdv{\\hat{y}^n_k}{z_i} \\right]\n",
    "\\end{align}\n",
    "We can now easily compute the full summation by excluding the term where $\\{i = k\\}$ and adding that separately:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{z_{i}}&=-\\sum^K_{\\begin{array}{c}\n",
    "    k = 1\\\\\n",
    "    k\\neq i\n",
    "\\end{array}}\\left[ \\frac{y^n_k}{\\hat{y}^n_k } \\left(-\\hat{y}^n_{i}\\hat{y}^n_{k}\\right) \\right] +  \\frac{y^n_i}{\\hat{y}^n_i } \\hat{y}^n_{i}\\left(1-\\hat{y}^n_{i}\\right)\\\\\n",
    "&=-\\sum^K_{\\begin{array}{c}\n",
    "    k = 1\\\\\n",
    "    k\\neq i\n",
    "\\end{array}}\\left[-y^n_k\\hat{y}^n_{i}\\right] + y^n_i\\left(1-\\hat{y}^n_{i}\\right)\\\\\n",
    "&=\\sum^K_{\\begin{array}{c}\n",
    "    k = 1\\\\\n",
    "    k\\neq i\n",
    "\\end{array}}\\left[y^n_k\\hat{y}^n_{i}\\right] - y^n_i\\left(1-\\hat{y}^n_{i}\\right)\\\\\n",
    "&=\\sum^K_{\\begin{array}{c}\n",
    "    k = 1\\\\\n",
    "    k\\neq i\n",
    "\\end{array}}\\left[y^n_k\\hat{y}^n_{i}\\right] - y^n_i + y^n_i\\hat{y}^n_{i}\\\\\n",
    "&=\\sum^K_{k = 1}\\left[y^n_k\\hat{y}^n_{i}\\right] - y^n_i\\\\\n",
    "&=\\hat{y}^n_{i}\\sum^K_{k = 1}\\left[y^n_k\\right] - y^n_i\\\\\n",
    "\\end{align}\n",
    "\n",
    "The special case of $\\sum^K_{k = 1}\\left[y^n_k\\right]$ is the known sum of the softmax function on $K$ classes, namely the sum of probabilities, which of course is equal to $1$\n",
    "\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{z_{i}}&=\\hat{y}^n_{i}\\sum^K_{k = 1}\\left[y^n_k\\right] - y^n_i\\\\\n",
    "\\pdv{C^n(w)}{z_{i}}&=\\hat{y}^n_{i} - y^n_i\\\\\n",
    "\\end{align}\n",
    "And with that, we now only need the trivial\n",
    "\\begin{align}\n",
    "\\pdv{z_i}{w_{ij}}&=x_j\n",
    "\\end{align}\n",
    "And then we have the full chain:\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{w_{ij}}&=\\pdv{C^n(w)}{z_{i}}\\pdv{z_i}{w_{ij}} = \\left(\\hat{y}^n_{i} - y^n_i\\right)x_j\n",
    "\\end{align}\n",
    "which, if we reorder around a little grants us with\n",
    "\\begin{align}\n",
    "\\pdv{C^n(w)}{w_{ij}}&= -x_j\\left(y^n_i - \\hat{y}^n_{i}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "\n",
    "With early stopping enabled, training stops at step 75.\n",
    "\n",
    "With the batch shuffling enabled too, training seems to stop arbitrarily in the step range [65-195]\n",
    "\n",
    "Observing again how the train loss and validation loss progress, and approximately where they start showing indications of overfitting - which I would say is around step 600, I'd say the early stopping criteria is a little bit too sensitive.\n",
    "This is further indicated by the improved validation accuracy after the early stopping range ([65-195]) when early stopping is disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "The reduction of \"spikes\" in accuracy progression we see when dataset shuffling is enabled can be a result of several factors.\n",
    "\n",
    "My main hypothesis is that the gradient descent \"drifts\" of from the _ideal_ path every epoch. The reason simply being that the individual mini-batches don't represent the various extremities of the dataset.\n",
    "\n",
    "The reason we're only seeing 10 spikes, is because we only do validation steps every 5th step through training, while there are 28 steps per epoch - which means we only track the epoch start every 5th epoch ($x 28 \\text{ mod } 5 = 0\\implies x=5$), while in fact during training there's one spike for every epoch.\n",
    "\n",
    "To me this is a clear indication that my hypothesis is strong. The samples at the start of every epoch is partially \"unlearnt\" as the latter samples of the epoch are biased towards different niches.\n",
    "\n",
    "Using dataset shuffling, we circumvent this issue by continually - by the balance of probability - keep the mini-batches as representative of the whole dataset as possible.\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)\n",
    "![](task3b_softmax_train_loss_stochastic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)\n",
    "![](task3b_softmax_train_accuracy_stochastic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "No. Initially, there's no sign of overfitting. \n",
    "A typical indication of overfitting is that the training accuracy first of all surpasses the validation accuracy - and second of all - validation accuracy stagnates wile training accuracy continues to improve. Another very typical sign of overfitting is that the validation loss deviates from the training loss - where the mean validation loss slows down it's decrease compared to the training loss.\n",
    "\n",
    "In fact we find that the validation accuracy is surprisingly high compared to the training accuracy. The same also applies for the respective losses. Generally speaking, having a validation accuracy be higher than the training accuracy isn't unheard of, and can be seen as a win for us as the trainers of the model. However, with this significant a leap in accuracy from training to validation, it is highly likely there is a bug in the evaluation code - Yes, we apply a very healthy dose of pessimism when training, and this scenario simply is too good to be true. \n",
    "\n",
    "In this scenario, we haven't been careful enough with the data we use in evaluation, making it seem as if we're performing very very well on unseen samples. However, this isn't really the case...\n",
    "\n",
    "In particular, we use a predefined set of samples for evaluation - Doing so can be perfectly fine, and is often how we approach test and validation sets. However, we are only using a subset of the full set as our validation set. As soon as we make a sub-selection, there are two things we need to make sure of. The point of any validation set is to have a selection of samples from the problem domain (here being handwritten digits) that are distinct from the training set. Hence, the most important thing with such a set, is that it's 1. distinct from the training set, and 2. that it manifests the full domain.\n",
    "\n",
    "With our code, only the first of the two clauses are ensured. 2. is not upheld as we're only sampling 2000 samples from the end of the original mnist test set. As we were not the ones assembling the test set, we cannot be sure the samples are fairly spanning the domain to its fullest. For all we know, the 2000 last samples of the test set represents digits only written by one person, biasing the validation to said person's vision of what \"true digits\" look like. Or even worse, we might think we sample 2000 samples uniformly spanning the 10 different classes of the mnist dataset, while in fact we only get 1000 zeros and 1000 ones.\n",
    "\n",
    "The latter of those two cases, I tested in code, by summing the one-hot encoded labels of the validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   |   0   |   1   |   2   |   3   |   4   |   5   |   6   |   7   |   8   |   9   |\n",
    "|---|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| % | 10.35 | 11.50 | 09.90 | 10.35 | 09.70 | 08.45 | 10.10 | 10.75 | 09.35 | 09.55 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems within reason in terms of class stability in the validation set. But we have no way of ensuring that the individual samples themselves don't tend to some niche of the _hand written digit domain_.\n",
    "\n",
    "Hence, we deemed it worth the while to implement a stochastic sampling variation of dataset loading. Where we still sample only 2000 samples, but they are selected at random without replacement across the 10000 available samples in the original test set. Doing so should in theory aid in representing the full span of the test set, while it also, probabilistically keeps the distribution of classes similar to that of the whole test set too. Above, we've included the plots with and without stochastic validation set selection. Where we in the stochastic one see a slight tendency towards overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\\begin{align}\n",
    "\\pdv{J(w)}{w}\n",
    "&= \\pdv{C(w)}{w} + \\lambda\\pdv{R(w)}{w}\\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n=1}\\left[\\pdv{C^n(w)}{w}\\right] + \\lambda\\pdv{\\| w \\|^2}{w}\\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n=1}\\left[-x^n\\left(y^n - \\hat{y}^n\\right)\\right] + \\lambda\\pdv{w^\\text{T}w}{w}\\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n=1}\\left[-x^n\\left(y^n - \\hat{y}^n\\right)\\right] + 2\\lambda w\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "Using regularization, we get a model that is penalised to have smaller weights overall, leading to the scenario where the weights cannot overfit to niche features of the dataset subset of the domain. The model is forced to be more generalised in its weight representation, which in effect reduces dataset specific \"noise\" which is otherwised encoded into the weights of the network, as can be seen:\n",
    "\n",
    "![](task4b_softmax_l2_regularization.png)\n",
    "\n",
    "Top row: model weights when $L2$ regularization factor $\\lambda = 0.0$\n",
    "\n",
    "Bottom row: model weights when $L2$ regularization factor $\\lambda = 2.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "While regularization is applied to help the model generalize, it also forces the model to work within tighter constraints, meaning that for any given set of data, we'll be forcing the model to limit how it uses the data.\n",
    "The higher the degree of the regularization factor is, the less of the extremeties of the dataset can be encoded within the weights. That is, the further we restrict the model to generalise, the worse it'll perform in outlier cases of data. This is known as the _bias-variance tradeof_. Increasing $\\lambda$ is equivalent to reducing variance and increasing bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4d_l2_reg_norms.png)\n",
    "\n",
    "We observe that the $L_2$ norm of the weights is approximately inversely proportional to $\\lambda$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}